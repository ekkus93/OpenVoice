{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6ee1ede",
   "metadata": {},
   "source": [
    "## Voice Style Control Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f043ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from openvoice import se_extractor\n",
    "from openvoice.api import BaseSpeakerTTS, ToneColorConverter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be106fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt: Prompt user for a Huggingface access token and save it as an environment variable\n",
    "\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "# Prompt user for Hugging Face access token\n",
    "hf_token = getpass.getpass('Enter your Hugging Face access token: ')\n",
    "\n",
    "# Save the token as an environment variable\n",
    "os.environ['HF_TOKEN'] = hf_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15116b59",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4d7af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacad912",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_base = 'checkpoints/base_speakers/EN'\n",
    "ckpt_converter = 'checkpoints/converter'\n",
    "device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "output_dir = 'outputs'\n",
    "\n",
    "base_speaker_tts = BaseSpeakerTTS(f'{ckpt_base}/config.json', device=device)\n",
    "base_speaker_tts.load_ckpt(f'{ckpt_base}/checkpoint.pth')\n",
    "\n",
    "tone_color_converter = ToneColorConverter(f'{ckpt_converter}/config.json', device=device)\n",
    "tone_color_converter.load_ckpt(f'{ckpt_converter}/checkpoint.pth')\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f67740c",
   "metadata": {},
   "source": [
    "### Obtain Tone Color Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8add279",
   "metadata": {},
   "source": [
    "The `source_se` is the tone color embedding of the base speaker. \n",
    "It is an average of multiple sentences generated by the base speaker. We directly provide the result here but\n",
    "the readers feel free to extract `source_se` by themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ff6273",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_se = torch.load(f'{ckpt_base}/en_default_se.pth').to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f71fcc3",
   "metadata": {},
   "source": [
    "The `reference_speaker.mp3` below points to the short audio clip of the reference whose voice we want to clone. We provide an example here. If you use your own reference speakers, please **make sure each speaker has a unique filename.** The `se_extractor` will save the `targeted_se` using the filename of the audio and **will not automatically overwrite.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55105eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reference_speaker = 'resources/example_reference.mp3' # This is the voice you want to clone\n",
    "reference_speaker = 'resources/fiona_zh.m4a'\n",
    "target_se, audio_name = se_extractor.get_se(reference_speaker, tone_color_converter, target_dir='processed', vad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40284aa",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dc1259",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = f'{output_dir}/output_en_default.wav'\n",
    "\n",
    "# Run the base speaker tts\n",
    "text = \"This audio is generated by OpenVoice.\"\n",
    "src_path = f'{output_dir}/tmp.wav'\n",
    "base_speaker_tts.tts(text, src_path, speaker='default', language='English', speed=1.0)\n",
    "\n",
    "# Run the tone color converter\n",
    "encode_message = \"@MyShell\"\n",
    "tone_color_converter.convert(\n",
    "    audio_src_path=src_path, \n",
    "    src_se=source_se, \n",
    "    tgt_se=target_se, \n",
    "    output_path=save_path,\n",
    "    message=encode_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c40a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "print(save_path)\n",
    "\n",
    "# Path to your .wav file\n",
    "Audio(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3ea28a",
   "metadata": {},
   "source": [
    "**Try with different styles and speed.** The style can be controlled by the `speaker` parameter in the `base_speaker_tts.tts` method. Available choices: friendly, cheerful, excited, sad, angry, terrified, shouting, whispering. Note that the tone color embedding need to be updated. The speed can be controlled by the `speed` parameter. Let's try whispering with speed 0.9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5abaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd022d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_se = torch.load(f'checkpoints/base_speakers/EN/en_style_se.pth').to(device)\n",
    "save_path = f'{output_dir}/output_whispering.wav'\n",
    "\n",
    "# Run the base speaker tts\n",
    "text = \"This audio is generated by OpenVoice.\"\n",
    "src_path = f'{output_dir}/tmp.wav'\n",
    "base_speaker_tts.tts(text, src_path, speaker='whispering', language='English', speed=0.9)\n",
    "\n",
    "# Run the tone color converter\n",
    "encode_message = \"@MyShell\"\n",
    "tone_color_converter.convert(\n",
    "    audio_src_path=src_path, \n",
    "    src_se=source_se, \n",
    "    tgt_se=target_se, \n",
    "    output_path=save_path,\n",
    "    message=encode_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0f6b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "print(save_path)\n",
    "\n",
    "# Path to your .wav file\n",
    "Audio(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcfc70b",
   "metadata": {},
   "source": [
    "**Try with different languages.** OpenVoice can achieve multi-lingual voice cloning by simply replace the base speaker. We provide an example with a Chinese base speaker here and we encourage the readers to try `demo_part2.ipynb` for a detailed demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71d1387",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ckpt_base = 'checkpoints/base_speakers/ZH'\n",
    "base_speaker_tts = BaseSpeakerTTS(f'{ckpt_base}/config.json', device=device)\n",
    "base_speaker_tts.load_ckpt(f'{ckpt_base}/checkpoint.pth')\n",
    "\n",
    "source_se = torch.load(f'{ckpt_base}/zh_default_se.pth').to(device)\n",
    "save_path = f'{output_dir}/output_chinese.wav'\n",
    "\n",
    "# Run the base speaker tts\n",
    "text = \"今天天气真好，我们一起出去吃饭吧。\"\n",
    "src_path = f'{output_dir}/tmp.wav'\n",
    "base_speaker_tts.tts(text, src_path, speaker='default', language='Chinese', speed=1.0)\n",
    "\n",
    "# Run the tone color converter\n",
    "encode_message = \"@MyShell\"\n",
    "tone_color_converter.convert(\n",
    "    audio_src_path=src_path, \n",
    "    src_se=source_se, \n",
    "    tgt_se=target_se, \n",
    "    output_path=save_path,\n",
    "    message=encode_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6233ecad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "print(save_path)\n",
    "\n",
    "# Path to your .wav file\n",
    "Audio(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e513094",
   "metadata": {},
   "source": [
    "**Tech for good.** For people who will deploy OpenVoice for public usage: We offer you the option to add watermark to avoid potential misuse. Please see the ToneColorConverter class. **MyShell reserves the ability to detect whether an audio is generated by OpenVoice**, no matter whether the watermark is added or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b03ea3",
   "metadata": {},
   "source": [
    "# A Small Incident"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39eb6f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一件小事⑴\n",
    "# 鲁迅\n",
    "# From https://www.comp.nus.edu.sg/~tanhw/chinese/literature/lu-xun/yi-jian-xiao-shi.html?utm_source=chatgpt.com\n",
    "text_str = \"\"\"\n",
    "　　我从乡下跑到京城里，一转眼已经六年了。其间耳闻目睹的所谓国家大事\n",
    "，算起来也很不少；但在我心里，都不留什么痕迹，倘要我寻出这些事的影响\n",
    "来说，便只是增长了我的坏脾气，——老实说，便是教我一天比一天的看不起\n",
    "人。\n",
    "\n",
    "　　但有一件小事，却于我有意义，将我从坏脾气里拖开，使我至今忘记不得\n",
    "。\n",
    "\n",
    "　　这是民国六年的冬天，大北风刮得正猛，我因为生计关系，不得不一早在\n",
    "路上走。一路几乎遇不见人，好容易才雇定了一辆人力车，教他拉到Ｓ门去。\n",
    "不一会，北风小了，路上浮尘早已刮净，剩下一条洁白的大道来，车夫也跑得\n",
    "更快。刚近Ｓ门，忽而车把上带着一个人，慢慢地倒了。\n",
    "\n",
    "　　跌倒的是一个女人，花白头发，衣服都很破烂。伊从马路上突然向车前横\n",
    "截过来；车夫已经让开道，但伊的破棉背心没有上扣，微风吹着，向外展开，\n",
    "所以终于兜着车把。幸而车夫早有点停步，否则伊定要栽一个大斤斗，跌到头\n",
    "破血出了。\n",
    "\n",
    "　　伊伏在地上；车夫便也立住脚。我料定这老女人并没有伤，又没有别人看\n",
    "见，便很怪他多事，要自己惹出是非，也误了我的路。\n",
    "\n",
    "　　我便对他说，“没有什么的。走你的罢！”\n",
    "\n",
    "　　车夫毫不理会，——或者并没有听到，——却放下车子，扶那老女人慢慢\n",
    "起来，搀着臂膊立定，问伊说：\n",
    "\n",
    "　　“你怎么啦？”\n",
    "\n",
    "　　“我摔坏了。”\n",
    "\n",
    "　　我想，我眼见你慢慢倒地，怎么会摔坏呢，装腔作势罢了，这真可憎恶。\n",
    "车夫多事，也正是自讨苦吃，现在你自己想法去。\n",
    "\n",
    "　　车夫听了这老女人的话，却毫不踌躇，仍然搀着伊的臂膊，便一步一步的\n",
    "向前走。我有些诧异，忙看前面，是一所巡警分驻所，大风之后，外面也不见\n",
    "人。这车夫扶着那老女人，便正是向那大门走去。\n",
    "\n",
    "　　我这时突然感到一种异样的感觉，觉得他满身灰尘的后影，刹时高大了，\n",
    "而且愈走愈大，须仰视才见。而且他对于我，渐渐的又几乎变成一种威压，甚\n",
    "而至于要榨出皮袍下面藏着的“小”来。\n",
    "\n",
    "　　我的活力这时大约有些凝滞了，坐着没有动，也没有想，直到看见分驻所\n",
    "里走出一个巡警，才下了车。\n",
    "\n",
    "　　巡警走近我说，“你自己雇车罢，他不能拉你了。”\n",
    "\n",
    "　　我没有思索的从外套袋里抓出一大把铜元，交给巡警，说，“请你给他…\n",
    "…”\n",
    "\n",
    "　　风全住了，路上还很静。我走着，一面想，几乎怕敢想到自己。以前的事\n",
    "姑且搁起，这一大把铜元又是什么意思？奖他么？我还能裁判车夫么？我不能\n",
    "回答自己。\n",
    "\n",
    "　　这事到了现在，还是时时记起。我因此也时时煞了苦痛，努力的要想到我\n",
    "自己。几年来的文治武力，在我早如幼小时候所读过的“子曰诗云”⑵一般，\n",
    "背不上半句了。独有这一件小事，却总是浮在我眼前，有时反更分明，教我惭\n",
    "愧，催我自新，并且增长我的勇气和希望。\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f8c68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def split_chinese_text_into_paragraphs_and_sentences(text):\n",
    "    \"\"\"\n",
    "    Splits Chinese text into paragraphs and then sentences within each paragraph.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input Chinese text.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of lists, where each inner list contains sentences from a paragraph.\n",
    "    \"\"\"\n",
    "    # Normalize full-width spaces and strip leading/trailing whitespace\n",
    "    text = text.replace('\\u3000', '').strip()\n",
    "\n",
    "    # Split into paragraphs by two or more newlines\n",
    "    paragraphs = re.split(r'\\n\\s*\\n', text)\n",
    "\n",
    "    paragraph_sentences = []\n",
    "    for paragraph in paragraphs:\n",
    "        # Remove excess internal newlines within paragraphs\n",
    "        paragraph = paragraph.replace('\\n', '')\n",
    "        # Split into sentences by Chinese punctuation (keep the punctuation)\n",
    "        sentences = re.split(r'(?<=[。！？])', paragraph)\n",
    "        # Remove empty strings and strip whitespace\n",
    "        sentences = [s.strip() for s in sentences if s.strip()]\n",
    "        paragraph_sentences.append(sentences)\n",
    "\n",
    "    return paragraph_sentences\n",
    "\n",
    "# Example usage:\n",
    "sentences_by_paragraph = split_chinese_text_into_paragraphs_and_sentences(text_str)\n",
    "\n",
    "# Display the results\n",
    "for i, paragraph in enumerate(sentences_by_paragraph):\n",
    "    print(f\"Paragraph {i+1}:\")\n",
    "    for j, sentence in enumerate(paragraph):\n",
    "        print(f\"  Sentence {j+1}: {sentence}\")\n",
    "    print(\"-\" * 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfc872a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'outputs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb21885",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reference_speaker = 'resources/example_reference.mp3' # This is the voice you want to clone\n",
    "reference_speaker = 'resources/fiona_zh.m4a'\n",
    "target_se, audio_name = se_extractor.get_se(reference_speaker, tone_color_converter, target_dir='processed', vad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a105b95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_se = torch.load(f'{ckpt_base}/zh_default_se.pth').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082d8a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_audio(save_path, text, output_dir, source_se, target_se):\n",
    "    src_path = f'{output_dir}/tmp.wav'\n",
    "    base_speaker_tts.tts(text, src_path, speaker='default', language='Chinese', speed=1.0)\n",
    "    \n",
    "    print(\"###save_path: \", save_path)\n",
    "\n",
    "    # Run the tone color converter\n",
    "    encode_message = \"@MyShell\"\n",
    "    tone_color_converter.convert(\n",
    "        audio_src_path=src_path, \n",
    "        src_se=source_se, \n",
    "        tgt_se=target_se, \n",
    "        output_path=save_path,\n",
    "        message=encode_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ebdc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchaudio as ta\n",
    "from pydub import AudioSegment\n",
    "\n",
    "# Create directory for sentence wav files\n",
    "output_dir = \"sentences\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Synthesize each sentence and save to a file\n",
    "sentence_files = []\n",
    "sentence_count = 0\n",
    "for paragraph_index, paragraph_sentences in enumerate(sentences_by_paragraph):\n",
    "    for sentence_index, sentence in enumerate(paragraph_sentences):\n",
    "        # Ensure sentence is not empty after splitting\n",
    "        if sentence.strip():\n",
    "            sentence_count += 1\n",
    "            print(f\"Synthesizing sentence {sentence_count}: {sentence}\")\n",
    "            sentence = sentence.strip()\n",
    "            \n",
    "            file_path = os.path.join(output_dir, f\"sentence_{sentence_count}.wav\")\n",
    "            \n",
    "            generate_audio(file_path, sentence, output_dir, source_se, target_se)\n",
    "            \n",
    "            sentence_files.append(file_path)\n",
    "\n",
    "# Merge all sentence wav files\n",
    "merged_audio = None\n",
    "pause_duration_ms = 500  # Adjust the pause duration as needed (in milliseconds)\n",
    "paragraph_end_pause_ms = 1000 # Pause duration after each paragraph\n",
    "\n",
    "file_index = 0\n",
    "sentence_counter_for_paragraph = 0\n",
    "\n",
    "for paragraph_index, paragraph_sentences in enumerate(sentences_by_paragraph):\n",
    "    sentence_counter_for_paragraph = 0\n",
    "    for sentence_index, sentence in enumerate(paragraph_sentences):\n",
    "        if sentence.strip():\n",
    "            file_path = sentence_files[file_index]\n",
    "            audio_segment = AudioSegment.from_wav(file_path)\n",
    "\n",
    "            if merged_audio is None:\n",
    "                merged_audio = audio_segment\n",
    "            else:\n",
    "                merged_audio += audio_segment\n",
    "\n",
    "            file_index += 1\n",
    "            sentence_counter_for_paragraph += 1\n",
    "\n",
    "    # Add a pause after each paragraph (if it's not the last paragraph)\n",
    "    if paragraph_index < len(sentences_by_paragraph) - 1:\n",
    "         # Add a pause at the end of the paragraph\n",
    "         pause = AudioSegment.silent(duration=paragraph_end_pause_ms)\n",
    "         if merged_audio is not None:\n",
    "            merged_audio += pause\n",
    "\n",
    "\n",
    "# Save the final merged audio\n",
    "if merged_audio is not None:\n",
    "    output_filename = \"A_Small_Incident.wav\"\n",
    "    merged_audio.export(output_filename, format=\"wav\")\n",
    "    print(f\"Merged audio saved as {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2bd41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "Audio('A_Small_Incident.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e9a17b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9d70c38e1c0b038dbdffdaa4f8bfa1f6767c43760905c87a9fbe7800d18c6c35"
  },
  "kernelspec": {
   "display_name": "openvoice",
   "language": "python",
   "name": "condaenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
